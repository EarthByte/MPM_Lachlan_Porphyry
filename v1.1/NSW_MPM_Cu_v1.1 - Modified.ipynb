{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49891b0",
   "metadata": {},
   "source": [
    "# Machine learning-based framework for prospectivity mapping of critical minerals\n",
    "\n",
    "### Ehsan Farahbakhsh, R. Dietmar M&uuml;ller\n",
    "\n",
    "*EarthByte Group, School of Geosciences, University of Sydney, NSW 2006, Australia*\n",
    "\n",
    "This notebook enables the user to create a prospectivity map of critical minerals in New South Wales, particularly the Lachlan Orogen. It comprises two main sections; in the first section, the available datasets are visualised, and in the second section, machine learning algorithms are applied to create a prospectivity map.\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbfaec1-93a2-496d-8866-4fbb14530ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "from ipywidgets import interact\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "\n",
    "from osgeo import gdal, osr\n",
    "from pyproj import CRS, Geod\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "\n",
    "from shapely.geometry import Point, LineString, MultiLineString\n",
    "from shapely.ops import nearest_points\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "from skimage import exposure, util\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Categorical, Integer, Real\n",
    "\n",
    "from pulearn import BaggingPuClassifier\n",
    "\n",
    "import cmcrameri.cm as cmc\n",
    "\n",
    "from typing import List, Literal, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdeb839",
   "metadata": {},
   "source": [
    "### Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ = gpd.read_file('./Dataset/Mineral Occurrences/porphyry.shp')\n",
    "# frame_target = nsw_bndy = gpd.read_file('./Dataset/Frames/NSW_Boundary.shp')\n",
    "frame_Lchn = gpd.read_file('./Dataset/Frames/Lachlan_Boundary.shp')\n",
    "frame_target = gpd.read_file('./Dataset/Frames/NSW_Boundary.shp')\n",
    "\n",
    "bounds = frame_target.bounds\n",
    "extent = [bounds.loc[0]['minx'], bounds.loc[0]['maxx'], bounds.loc[0]['miny'], bounds.loc[0]['maxy']]\n",
    "\n",
    "bounds_target = frame_target.bounds\n",
    "extent_target = [bounds_target.loc[0]['minx'], bounds_target.loc[0]['maxx'], bounds_target.loc[0]['miny'], bounds_target.loc[0]['maxy']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.5)\n",
    "frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Porphyry Mineral Occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc639b4b",
   "metadata": {},
   "source": [
    "### Vector Data Layers\n",
    "\n",
    "#### Intrusion Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e25953",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrusion_bndy_files = [\n",
    "    './Dataset/Geology/Polylines/IntrusionsBndys_FaultedBndys.shp',\n",
    "    './Dataset/Geology/Polylines/IntrusionsBndys_GeologicalBndys.shp',\n",
    "    './Dataset/Geology/Polylines/IntrusionsBndys_IntrusiveBndys.shp',\n",
    "    './Dataset/Geology/Polylines/IntrusionsBndys_UnconformableBndys.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=intrusion_bndy_files)\n",
    "\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Intrusion Boundaries')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fffb9",
   "metadata": {},
   "source": [
    "#### Metamorphic Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a998dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_bndy_files = [\n",
    "    './Dataset/Geology/Polylines/MetamorphicBoundaries_Faults.shp',\n",
    "    './Dataset/Geology/Polylines/MetamorphicBoundaries_GeologicalBndys.shp',\n",
    "    './Dataset/Geology/Polylines/MetamorphicBoundaries_MetamorphicBndys.shp',\n",
    "    './Dataset/Geology/Polylines/MetamorphicBoundaries_Unconformities.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=metamorphic_bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Metamorphic Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee4920b",
   "metadata": {},
   "source": [
    "#### Metamorphic Isograds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c86fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metamorphic_iso_file = './Dataset/Geology/Polylines/MetamorphicIsograds.shp'\n",
    "metamorphic_iso = gpd.read_file(metamorphic_iso_file)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "metamorphic_iso.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Metamorphic Isograds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bd038",
   "metadata": {},
   "source": [
    "#### Rock Units, Boundaries, and Fault Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "bndy_files = [\n",
    "    './Dataset/Geology/Polylines/RockUnitBndyFaults_LAO_Faulted boundary.shp',\n",
    "    './Dataset/Geology/Polylines/RockUnitBndyFaults_LAO_Geological boundary.shp',\n",
    "    './Dataset/Geology/Polylines/RockUnitBndyFaults_LAO_Intrusive boundary.shp',\n",
    "    './Dataset/Geology/Polylines/RockUnitBndyFaults_LAO_Unconformable boundary.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=bndy_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    data.plot(ax=ax, edgecolor='none', color='blue', linewidth=1)\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.set_title('Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bcf67b-b2cb-4ec2-8052-9b5c6e4384f3",
   "metadata": {},
   "source": [
    "#### Intrusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c1d13-0126-4fcc-b81e-f33cc7685cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile\n",
    "intrusions_file = './Dataset/Geology/Polygons/Intrusions_Tabberabberan.shp'\n",
    "intrusions = gpd.read_file(intrusions_file)\n",
    "\n",
    "# Ensure there is a column to differentiate the geological units, for example 'unit'\n",
    "# If 'unit' does not exist, replace 'unit' with the appropriate column name\n",
    "intrusions_units = sorted(intrusions['Dominant_L'].unique()) # Sort the units alphabetically\n",
    "\n",
    "# Generate a colorblind-friendly palette with cmc\n",
    "cmap = cmc.roma  # Use the 'roma' colormap from cmcrameri\n",
    "palette = cmap(np.linspace(0, 1, len(intrusions_units)))\n",
    "\n",
    "# Create a dictionary mapping each geological unit to a unique color\n",
    "unit_colors = {unit: palette[i] for i, unit in enumerate(intrusions_units)}\n",
    "\n",
    "# Plot the shapefile with the assigned colors\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "for unit, color in unit_colors.items():\n",
    "    subset = intrusions[intrusions['Dominant_L'] == unit]\n",
    "    subset.plot(ax=ax, color=color)\n",
    "\n",
    "frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "\n",
    "patches = [mpatches.Patch(color=color, label=unit) for unit, color in unit_colors.items()]\n",
    "\n",
    "# Define font properties for the legend title\n",
    "title_font = FontProperties(weight='bold', size=12)\n",
    "\n",
    "# Create custom legend (inside)\n",
    "plt.legend(handles=patches, loc='lower right', title='Dominant Lithology',\n",
    "          title_fontproperties=title_font)\n",
    "\n",
    "# Add a scale bar\n",
    "scalebar = ScaleBar(95, units='km', dimension='si-length', location='lower left', length_fraction=0.2)  # Adjust units as needed\n",
    "# For UTM based coordinate system, where the X and Y are in meters, simply set dx = 1.\n",
    "# For WGS or NAD based coordinate system, where X and Y are in latitude (Y) and longitude (X),\n",
    "# compute the distance between two points at the latitude (Y) you wish to have the scale represented and are also one\n",
    "# full degree of longitude (X) apart, in meters. For example, dx = great_circle_distance((X, Y), (X + 1, Y))\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "plt.title('Intrusions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5478f7b-4ebc-4044-a69a-eccbcec3bb1a",
   "metadata": {},
   "source": [
    "#### Metamorphic Facies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991bc86-7dfe-4691-9039-d46a49220e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fac_files = [\n",
    "    './Dataset/Geology/Polygons/MetamorphicFacies_Benambran.shp',\n",
    "    './Dataset/Geology/Polygons/MetamorphicFacies_KanimblanTablelands.shp',\n",
    "    './Dataset/Geology/Polygons/MetamorphicFacies_Tabberabberan.shp'\n",
    "]\n",
    "\n",
    "@interact(dataset=meta_fac_files)\n",
    "def show_dist(dataset):\n",
    "    data = gpd.read_file(dataset)\n",
    "    data_units = sorted(data['MetFacies'].unique())\n",
    "    cmap = cmc.roma\n",
    "    palette = cmap(np.linspace(0, 1, len(data_units)))\n",
    "    unit_colors = {unit: palette[i] for i, unit in enumerate(data_units)}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    for unit, color in unit_colors.items():\n",
    "        subset = data[data['MetFacies'] == unit]\n",
    "        subset.plot(ax=ax, color=color)\n",
    "\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "    min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    \n",
    "    patches = [mpatches.Patch(color=color, label=unit) for unit, color in unit_colors.items()]\n",
    "\n",
    "    title_font = FontProperties(weight='bold', size=12)\n",
    "\n",
    "    plt.legend(handles=patches, loc='lower right', title='Metamorphic Facies',\n",
    "              title_fontproperties=title_font)\n",
    "\n",
    "    scalebar = ScaleBar(95, units='km', dimension='si-length', location='lower left', length_fraction=0.2)\n",
    "    ax.add_artist(scalebar)\n",
    "\n",
    "    plt.title('Metamorphic Facies')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2c997-4c63-4005-ad68-78b49f6e050f",
   "metadata": {},
   "source": [
    "#### Rock Units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb26303-ccfe-4ace-9371-ddba9a1cf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rock_units_file = './Dataset/Geology/Polygons/RockUnits_LAO.shp'\n",
    "rock_units = gpd.read_file(rock_units_file)\n",
    "\n",
    "rock_units_ = sorted(rock_units['Dominant_L'].unique())\n",
    "\n",
    "cmap = cmc.roma\n",
    "palette = cmap(np.linspace(0, 1, len(rock_units_)))\n",
    "\n",
    "unit_colors = {unit: palette[i] for i, unit in enumerate(rock_units_)}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "for unit, color in unit_colors.items():\n",
    "    subset = rock_units[rock_units['Dominant_L'] == unit]\n",
    "    subset.plot(ax=ax, color=color)\n",
    "\n",
    "frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "\n",
    "patches = [mpatches.Patch(color=color, label=unit) for unit, color in unit_colors.items()]\n",
    "\n",
    "title_font = FontProperties(weight='bold', size=12)\n",
    "\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1, 0.72), loc='upper left', title='Dominant Lithology',\n",
    "          title_fontproperties=title_font, ncol=2)\n",
    "\n",
    "scalebar = ScaleBar(95, units='km', dimension='si-length', location='lower left', length_fraction=0.2)\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "plt.title('Rock Units')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c73475-6b7b-4524-a5ad-63a61d5cad20",
   "metadata": {},
   "source": [
    "### Raster Data Layers \n",
    "\n",
    "#### Magnetic Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc3345-92a5-46d4-8490-d0ff010595f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnetic_files = [\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_1vd-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_05vd-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_as-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Bzz-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGrav-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PGravTHD-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-Phase-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_enhancement-PSusp-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC0m500mRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC1km2kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC2km4kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC4km8kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC8km12kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC12km16kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC16km20kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC24km30kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC36km42kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp_upcon-UC42km50kmRes-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi_rtp-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi-AWAGS_MAG_2019.tif',\n",
    "    './Dataset/Magnetic/Magmap2019-grid-tmi-Cellsize40m-AWAGS_MAG_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=magnetic_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != frame_target.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(frame_target.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap=cmc.roma, extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcae7a53-0881-48c0-8b08-c1045239a92c",
   "metadata": {},
   "source": [
    "#### Gravity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4532a035-14bb-4c47-8627-3317ae5d807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity_files = [\n",
    "    './Dataset/Gravity/Gravmap2016-grid-grv_cscba.tif',\n",
    "    './Dataset/Gravity/Gravmap2016-grid-grv_ir.tif',\n",
    "    './Dataset/Gravity/Gravmap2016-grid-grv_scba.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_1vd.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_1vd-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_05vd.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_05vd-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_tilt.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba_tilt-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_cscba-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_1vd.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_1vd-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_05vd.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_05vd-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_tilt.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir_tilt-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_dtgir-IncludesAirborne.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_fa.tif',\n",
    "    './Dataset/Gravity/Gravmap2019-grid-grv_fa-IncludesAirborne.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=gravity_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != frame_target.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(frame_target.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap=cmc.roma, extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd495a5-90ae-4e31-83e2-4d40fc03cd0b",
   "metadata": {},
   "source": [
    "#### Radiometric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603833a0-79fa-4494-b58d-73fa521b6fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "radiometric_files = [\n",
    "    './Dataset/Radiometric/Radmap2019-grid-dose_terr-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-dose_terr-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-k_conc-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-k_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-th_conc-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-th_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-thk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-u_conc-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-u_conc-Filtered-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-u2th_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-uk_ratio-AWAGS_RAD_2019.tif',\n",
    "    './Dataset/Radiometric/Radmap2019-grid-uth_ratio-AWAGS_RAD_2019.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=radiometric_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != frame_target.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(frame_target.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap=cmc.roma, extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4193ed-8b39-415e-94b2-52240baad0be",
   "metadata": {},
   "source": [
    "#### Remote Sensing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d1f6d-f87b-4bfe-9042-6c80adf22cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_sensing_files = [\n",
    "    './Dataset/Remote Sensing/AlOH_Group_Composition.tif',\n",
    "    './Dataset/Remote Sensing/AlOH_Group_Content.tif',\n",
    "    './Dataset/Remote Sensing/FeOH_Group_Content.tif',\n",
    "    './Dataset/Remote Sensing/Ferric_Oxide_Composition.tif',\n",
    "    './Dataset/Remote Sensing/Ferric_Oxide_Content.tif',\n",
    "    './Dataset/Remote Sensing/Ferrous_Iron_Content_in_MgOH.tif',\n",
    "    './Dataset/Remote Sensing/Ferrous_Iron_Index.tif',\n",
    "    './Dataset/Remote Sensing/Green_Vegetation.tif',\n",
    "    './Dataset/Remote Sensing/Gypsum_Index.tif',\n",
    "    './Dataset/Remote Sensing/Kaolin_Group_Index.tif',\n",
    "    './Dataset/Remote Sensing/MgOH_Group_Composition.tif',\n",
    "    './Dataset/Remote Sensing/MgOH_Group_Content.tif',\n",
    "    './Dataset/Remote Sensing/Opaque_Index.tif',\n",
    "    './Dataset/Remote Sensing/Quartz_Index.tif',\n",
    "    './Dataset/Remote Sensing/Silica_Index.tif'\n",
    "    ]\n",
    "\n",
    "@interact(file=remote_sensing_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != frame_target.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(frame_target.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap=cmc.roma, extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    filename = os.path.splitext(os.path.basename(file))[0]\n",
    "    plt.colorbar(cb, orientation='horizontal', label=filename, cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc701f-ae23-417f-b742-481293b89b4b",
   "metadata": {},
   "source": [
    "#### Elevation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba1d1f6-3077-469d-b14c-46c981ef9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_files = [\n",
    "    './Dataset/Elevation/Gravmap2019-grid-ausdrape_ellips.tif',\n",
    "    './Dataset/Elevation/Gravmap2019-grid-ausdrape_geoid.tif',\n",
    "    './Dataset/Elevation/Gravmap2019-grid-dem_ellips.tif',\n",
    "    './Dataset/Elevation/Gravmap2019-grid-dem_geoid.tif',\n",
    "    ]\n",
    "\n",
    "@interact(file=elevation_files)\n",
    "def show_dist(file):\n",
    "    raster = rxr.open_rasterio(file, masked=True).squeeze()\n",
    "    \n",
    "    # reproject if required\n",
    "    if raster.rio.crs.to_epsg() != frame_target.crs.to_epsg():\n",
    "        raster = raster.rio.reproject(frame_target.crs)\n",
    "    \n",
    "    raster_array = raster.values\n",
    "    v_mean = np.nanmean(raster_array)\n",
    "    v_std = np.nanstd(raster_array)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    cb = ax.imshow(raster_array, cmap=cmc.roma, extent=extent, vmin=v_mean-v_std, vmax=v_mean+v_std)\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    plt.colorbar(cb, orientation='horizontal', label='Elevation (m)', cax=cax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7ac0e",
   "metadata": {},
   "source": [
    "### Extract the Coordinates of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba923ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the coordinates to a CSV file\n",
    "deposit_coords_file = './Dataset/Output/training_data_deposit_coords.csv'\n",
    "\n",
    "if os.path.isfile(deposit_coords_file):\n",
    "    print('The coordinates of deposits already exist.')\n",
    "    deposit_coords = pd.read_csv(deposit_coords_file, index_col=False)\n",
    "    deposit_num = int(deposit_coords.shape[0])\n",
    "    deposit_x = pd.Series.tolist(deposit_coords['X'])\n",
    "    deposit_y = pd.Series.tolist(deposit_coords['Y'])\n",
    "else:\n",
    "    deposit_x = min_occ.geometry.x\n",
    "    deposit_y = min_occ.geometry.y\n",
    "    deposit_num = min_occ.shape[0]\n",
    "    deposit_coords = pd.DataFrame(deposit_x, columns=['X'])\n",
    "    deposit_coords['Y'] = deposit_y\n",
    "    \n",
    "    size_code = pd.Series.tolist(min_occ['SIZE_CODE'])\n",
    "    sample_weight = []\n",
    "    for i in range(len(size_code)):\n",
    "        if size_code[i] == 'VLG':\n",
    "            sample_weight.append(0.5)\n",
    "        elif size_code[i] == 'LGE':\n",
    "            sample_weight.append(0.4)\n",
    "        elif size_code[i] == 'MED':\n",
    "            sample_weight.append(0.3)\n",
    "        elif size_code[i] == 'SML':\n",
    "            sample_weight.append(0.2)\n",
    "        elif size_code[i] == 'OCC':\n",
    "            sample_weight.append(0.1)\n",
    "        \n",
    "    deposit_coords['label'] = 1\n",
    "    deposit_coords['sample_weight'] = sample_weight\n",
    "    deposit_coords.to_csv(deposit_coords_file, index=False)\n",
    "    print(f'The coordinates of deposits have been saved to {deposit_coords_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3937e6",
   "metadata": {},
   "source": [
    "### Prepare the Training Data File\n",
    "\n",
    "#### Vector Data Layers\n",
    "\n",
    "##### Polylines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bd258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance from points to linear features\n",
    "def get_dist_line(xs: Union[List[float], np.ndarray], \n",
    "                  ys: Union[List[float], np.ndarray], \n",
    "                  line_files: List[str], \n",
    "                  distance_type: Literal['euclidean', 'geodesic'] = 'euclidean',\n",
    "                  input_crs: str = 'EPSG:4283') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the distance from points to linear features.\n",
    "    Args:\n",
    "    xs (Union[List[float], np.ndarray]): x-coordinates of the points\n",
    "    ys (Union[List[float], np.ndarray]): y-coordinates of the points\n",
    "    line_files (List[str]): a list of file paths to linear features\n",
    "    distance_type (str): type of distance calculation ('euclidean' or 'geodesic')\n",
    "    input_crs (str): Coordinate Reference System of input data\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame where each column represents distances to a line in meters\n",
    "    \"\"\"\n",
    "    if len(xs) != len(ys):\n",
    "        raise ValueError(\"xs and ys must have the same length\")\n",
    "    \n",
    "    if distance_type not in ['euclidean', 'geodesic']:\n",
    "        raise ValueError(\"distance_type must be either 'euclidean' or 'geodesic'\")\n",
    "\n",
    "    lines = []\n",
    "    column_names = []\n",
    "    \n",
    "    input_crs = CRS(input_crs)\n",
    "    \n",
    "    for file in line_files:\n",
    "        gdf = gpd.read_file(file)\n",
    "        if gdf.crs is None:\n",
    "            gdf.set_crs(input_crs, inplace=True)\n",
    "        lines.append(gdf)\n",
    "        column_names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    \n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=points, crs=input_crs)\n",
    "    \n",
    "    dist_to_lines = np.zeros((len(points), len(lines)))\n",
    "    \n",
    "    is_projected = input_crs.is_projected\n",
    "    \n",
    "    if distance_type == 'euclidean':\n",
    "        if not is_projected:\n",
    "            # Project to a suitable local projection\n",
    "            proj_crs = get_suitable_projected_crs(points_gdf)\n",
    "            points_gdf = points_gdf.to_crs(proj_crs)\n",
    "            lines = [line.to_crs(proj_crs) for line in lines]\n",
    "    else:  # geodesic\n",
    "        if is_projected:\n",
    "            # Convert to a geographic CRS for geodesic calculations\n",
    "            geo_crs = CRS(\"EPSG:4326\")\n",
    "            points_gdf = points_gdf.to_crs(geo_crs)\n",
    "            lines = [line.to_crs(geo_crs) for line in lines]\n",
    "        ellps = get_ellipsoid_name(points_gdf.crs)\n",
    "        geod = Geod(ellps=ellps)\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line_tree = STRtree(line.geometry)\n",
    "        nearest_geoms = [line_tree.nearest(p) for p in points_gdf.geometry]\n",
    "        \n",
    "        if distance_type == 'euclidean':\n",
    "            dist_to_lines[:, i] = [p.distance(nearest) for p, nearest in zip(points_gdf.geometry, nearest_geoms)]\n",
    "        else:  # geodesic\n",
    "            for j, (p, nearest) in enumerate(zip(points_gdf.geometry, nearest_geoms)):\n",
    "                nearest_point = get_nearest_point(p, nearest)\n",
    "                _, _, distance = geod.inv(p.x, p.y, nearest_point.x, nearest_point.y)\n",
    "                dist_to_lines[j, i] = distance\n",
    "    \n",
    "    return pd.DataFrame(dist_to_lines, columns=column_names)\n",
    "# concatenate and export the features generated using the functions above\n",
    "def get_vector_data(xs, ys):\n",
    "    lines_files = intrusion_bndy_files + metamorphic_bndy_files + bndy_files\n",
    "    lines_files.append(metamorphic_iso_file)\n",
    "    lines = []\n",
    "    column_names = []\n",
    "    \n",
    "    for file in lines_files:\n",
    "        lines.append(gpd.read_file(file))\n",
    "        column_names.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    \n",
    "    dist_lines_df = get_dist_lines(xs, ys, lines, column_names)\n",
    "    return dist_lines_df\n",
    "\n",
    "deposit_vector_file = f'./Dataset/Output/training_data_deposit_vector.csv'\n",
    "\n",
    "if os.path.isfile(deposit_vector_file):\n",
    "    print('The vector dataset (deposits) already exists.')\n",
    "    deposit_vector_data = pd.read_csv(deposit_vector_file, index_col=False)\n",
    "else:\n",
    "    deposit_vector_data = get_vector_data(deposit_x, deposit_y)\n",
    "    deposit_vector_data = deposit_vector_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_vector_data.to_csv(deposit_vector_file, index=False)\n",
    "    print(f'The vector dataset (deposits) has been saved to {deposit_vector_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f20108",
   "metadata": {},
   "source": [
    "##### Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract categorical data\n",
    "def get_cat_data(\n",
    "    xs: Union[List[float], np.ndarray],\n",
    "    ys: Union[List[float], np.ndarray],\n",
    "    polygon_files: Union[str, List[str]],\n",
    "    field: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract categorical data from geographic polygon files based on given point coordinates.\n",
    "\n",
    "    This function performs a spatial join between input points and polygons from specified files,\n",
    "    extracting a given field value for each point based on the polygon it falls within.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : Union[List[float], np.ndarray]\n",
    "        List or array of x-coordinates.\n",
    "    ys : Union[List[float], np.ndarray]\n",
    "        List or array of y-coordinates.\n",
    "    polygon_files : Union[str, List[str]]\n",
    "        Single file path or list of file paths to polygon shapefiles.\n",
    "    field : str\n",
    "        Name of the field in the polygon files to extract.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the extracted categorical data for each point and each polygon file.\n",
    "        Columns are named after the input files, rows correspond to input points.\n",
    "\n",
    "    Raises:\n",
    "    -------\n",
    "    ValueError\n",
    "        If input coordinates are not of equal length or if polygon_files is empty.\n",
    "    FileNotFoundError\n",
    "        If any of the specified polygon files does not exist.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Points that do not fall within any polygon will have 'Null' as their value.\n",
    "    - Any errors during file processing will be printed, and the file will be skipped.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if len(xs) != len(ys):\n",
    "        raise ValueError(\"Input coordinates must be of equal length\")\n",
    "    \n",
    "    # Convert polygon_files to a list if it's a single string\n",
    "    if isinstance(polygon_files, str):\n",
    "        polygon_files = [polygon_files]\n",
    "    \n",
    "    if not polygon_files:\n",
    "        raise ValueError(\"At least one polygon file must be specified\")\n",
    "    \n",
    "    for file in polygon_files:\n",
    "        if not os.path.exists(file):\n",
    "            raise FileNotFoundError(f\"Polygon file not found: {file}\")\n",
    "\n",
    "    # Create GeoDataFrame from input points\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=[Point(x, y) for x, y in zip(xs, ys)])\n",
    "    cat_code = pd.DataFrame()\n",
    "\n",
    "    for file in polygon_files:\n",
    "        column_name = os.path.splitext(os.path.basename(file))[0]\n",
    "        try:\n",
    "            polygon_gdf = gpd.read_file(file)\n",
    "            \n",
    "            # Ensure the field exists in the GeoDataFrame\n",
    "            if field not in polygon_gdf.columns:\n",
    "                print(f\"Warning: Field '{field}' not found in {column_name}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Spatial join\n",
    "            joined = gpd.sjoin(points_gdf, polygon_gdf, how=\"left\", predicate=\"within\")\n",
    "            \n",
    "            # Extract the field values, replacing NaN with 'Null'\n",
    "            cat_code[column_name] = joined[field].fillna('Null')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {column_name}: {str(e)}\")\n",
    "    \n",
    "    return cat_code\n",
    "\n",
    "# export the features generated using the functions above\n",
    "deposit_cat_file = f'./Dataset/Output/training_data_deposit_categorical.csv'\n",
    "\n",
    "if os.path.isfile(deposit_cat_file):\n",
    "    print('The categorical dataset (deposits) already exists.')\n",
    "    deposit_cat_data = pd.read_csv(deposit_cat_file, index_col=False)\n",
    "else:\n",
    "    deposit_cat_data = get_cat_data(deposit_x, deposit_y)\n",
    "    deposit_cat_data = deposit_cat_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_cat_data.to_csv(deposit_cat_file, index=False)\n",
    "    print(f'The categorical dataset (deposits) has been saved to {deposit_cat_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8648f",
   "metadata": {},
   "source": [
    "#### Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd43f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean and standard deviation in a buffer zone (circle) surrounding each target point\n",
    "def get_grid_stat_features(\n",
    "    xs: Union[List[float], np.ndarray],\n",
    "    ys: Union[List[float], np.ndarray],\n",
    "    grid_paths: Union[str, List[str]],\n",
    "    buffer_size: float,\n",
    "    buffer_shape: str = 'circular',\n",
    "    stats: List[str] = ['mean', 'std'],\n",
    "    gradient: Union[None, str, List[str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate specified statistics within a buffer around given points for one or multiple raster files.\n",
    "\n",
    "    Args:\n",
    "        xs (Union[List[float], np.ndarray]): X coordinates of the points.\n",
    "        ys (Union[List[float], np.ndarray]): Y coordinates of the points.\n",
    "        grid_paths (Union[str, List[str]]): Path(s) to the raster file(s).\n",
    "        buffer_size (float): Size of the buffer in meters (radius for circular, half side length for square).\n",
    "        buffer_shape (str, optional): Shape of the buffer, either 'circular' or 'square'. Defaults to 'circular'.\n",
    "        stats (List[str], optional): List of statistics to calculate. Defaults to ['mean', 'std'].\n",
    "        gradient (Union[None, str, List[str]], optional): Direction(s) to calculate gradient, can be None, 'x', 'y', 'both', or ['x', 'y']. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with specified statistic columns for each input raster and gradient type.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If buffer_shape is not 'circular' or 'square'.\n",
    "        ValueError: If the number of x and y coordinates don't match.\n",
    "        FileNotFoundError: If any of the specified raster files are not found.\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    if buffer_shape not in ['circular', 'square']:\n",
    "        raise ValueError(\"buffer_shape must be either 'circular' or 'square'\")\n",
    "    \n",
    "    if len(xs) != len(ys):\n",
    "        raise ValueError(\"The number of x and y coordinates must be the same\")\n",
    "\n",
    "    valid_gradients = [None, 'x', 'y', 'both', ['x', 'y']]\n",
    "    if gradient not in valid_gradients:\n",
    "        raise ValueError(\"gradient must be None, 'x', 'y', 'both', or ['x', 'y']\")\n",
    "\n",
    "    # Convert inputs to numpy arrays\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Ensure grid_paths is a list\n",
    "    if isinstance(grid_paths, str):\n",
    "        grid_paths = [grid_paths]\n",
    "\n",
    "    # Check if all files exist\n",
    "    for path in grid_paths:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Raster file not found: {path}\")\n",
    "\n",
    "    # Define statistic functions\n",
    "    stat_functions = {\n",
    "        'mean': np.nanmean,\n",
    "        'std': np.nanstd,\n",
    "        'min': np.nanmin,\n",
    "        'max': np.nanmax,\n",
    "        'median': np.nanmedian\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for grid_path in grid_paths:\n",
    "        # Open the raster file\n",
    "        raster = rxr.open_rasterio(grid_path, masked=True).squeeze()\n",
    "        bounds = raster.rio.bounds()\n",
    "        raster_np = raster.values\n",
    "\n",
    "        # Get the CRS of the raster\n",
    "        raster_crs = raster.rio.crs\n",
    "\n",
    "        # Prepare gradient rasters\n",
    "        rasters_to_process = [('original', raster_np)]\n",
    "        if gradient:\n",
    "            if gradient in ['x', 'both'] or (isinstance(gradient, list) and 'x' in gradient):\n",
    "                grad_x = np.gradient(raster_np, axis=1)\n",
    "                rasters_to_process.append(('grad_x', grad_x))\n",
    "            if gradient in ['y', 'both'] or (isinstance(gradient, list) and 'y' in gradient):\n",
    "                grad_y = np.gradient(raster_np, axis=0)\n",
    "                rasters_to_process.append(('grad_y', grad_y))\n",
    "            if gradient == 'both':\n",
    "                grad_both = np.sqrt(grad_x**2 + grad_y**2)\n",
    "                rasters_to_process.append(('grad_both', grad_both))\n",
    "\n",
    "        # Convert coordinates to pixel space\n",
    "        x_pixels = ((xs - bounds[0]) / (bounds[2] - bounds[0]) * (raster_np.shape[1] - 1)).astype(int)\n",
    "        y_pixels = ((ys - bounds[1]) / (bounds[3] - bounds[1]) * (raster_np.shape[0] - 1)).astype(int)\n",
    "\n",
    "        # Calculate pixel size\n",
    "        if raster_crs.is_projected:\n",
    "            pixel_size_x = (bounds[2] - bounds[0]) / raster_np.shape[1] * raster_crs.linear_units_factor\n",
    "            pixel_size_y = (bounds[3] - bounds[1]) / raster_np.shape[0] * raster_crs.linear_units_factor\n",
    "        else:\n",
    "            # Approximate pixel size for geographic CRS\n",
    "            pixel_size_x = (bounds[2] - bounds[0]) / raster_np.shape[1] * 111319.9  # approx meters per degree at equator\n",
    "            pixel_size_y = (bounds[3] - bounds[1]) / raster_np.shape[0] * 111319.9\n",
    "\n",
    "        # Calculate buffer size in pixels\n",
    "        pixels_per_meter = min(1/pixel_size_x, 1/pixel_size_y)\n",
    "        buffer_pixels = max(int(buffer_size * pixels_per_meter), 1)  # Ensure at least 1 pixel\n",
    "\n",
    "        for raster_type, raster_data in rasters_to_process:\n",
    "            stat_results = {stat: [] for stat in stats}\n",
    "\n",
    "            if buffer_shape == 'square':\n",
    "                for x, y in zip(x_pixels, y_pixels):\n",
    "                    x_min, x_max = max(0, x - buffer_pixels), min(raster_data.shape[1], x + buffer_pixels + 1)\n",
    "                    y_min, y_max = max(0, y - buffer_pixels), min(raster_data.shape[0], y + buffer_pixels + 1)\n",
    "                    \n",
    "                    values = raster_data[y_min:y_max, x_min:x_max]\n",
    "                    valid_values = values[~np.isnan(values) & ~np.isinf(values)]\n",
    "                    for stat in stats:\n",
    "                        if valid_values.size > 0:\n",
    "                            stat_results[stat].append(stat_functions[stat](valid_values))\n",
    "                        else:\n",
    "                            stat_results[stat].append(np.nan)\n",
    "            else:  # circular buffer\n",
    "                y_indices, x_indices = np.indices(raster_data.shape)\n",
    "                for x, y in zip(x_pixels, y_pixels):\n",
    "                    mask = (x_indices - x)**2 + (y_indices - y)**2 <= buffer_pixels**2\n",
    "                    values = raster_data[mask]\n",
    "                    valid_values = values[~np.isnan(values) & ~np.isinf(values)]\n",
    "                    for stat in stats:\n",
    "                        if valid_values.size > 0:\n",
    "                            stat_results[stat].append(stat_functions[stat](valid_values))\n",
    "                        else:\n",
    "                            stat_results[stat].append(np.nan)\n",
    "\n",
    "            # Create a prefix for column names\n",
    "            prefix = os.path.splitext(os.path.basename(grid_path))[0]\n",
    "            for stat in stats:\n",
    "                results[f'{prefix}_{raster_type}_{stat}'] = stat_results[stat]\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# calculate dissimilarity and correlation in a window (square) surrounding each target point\n",
    "def get_diss_corr(xs, ys, raster_np, bounds, patch_size):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    dissimilarity = []\n",
    "    correlation = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        \n",
    "        left = max(x-patch_size/2, bounds[0])\n",
    "        right = min(x+patch_size/2, bounds[2])\n",
    "        top = min(y+patch_size/2, bounds[3])\n",
    "        bottom = max(y-patch_size/2, bounds[1])\n",
    "        \n",
    "        left_idx = math.floor((left-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        right_idx = math.floor((right-bounds[0])/(bounds[2]-bounds[0])*(raster_np.shape[1]-1))\n",
    "        bottom_idx = math.floor((bottom-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "        top_idx = math.floor((top-bounds[1])/(bounds[3]-bounds[1])*(raster_np.shape[0]-1))\n",
    "\n",
    "        xs = np.arange(left_idx, right_idx+1)\n",
    "        ys = np.arange(raster_np.shape[0]-1-top_idx, raster_np.shape[0]-1-bottom_idx+1)\n",
    "        xm, ym = np.meshgrid(xs, ys)\n",
    "        \n",
    "        if np.isnan(raster_np[ym, xm]).all():\n",
    "            dissimilarity.append(np.nan)\n",
    "            correlation.append(np.nan)\n",
    "        else:\n",
    "            raster_scaled = exposure.rescale_intensity(raster_np[ym, xm], in_range=(np.nanmin(raster_np[ym, xm]), np.nanmax(raster_np[ym, xm])), out_range=(0, 1))\n",
    "            raster_ubyte = util.img_as_ubyte(raster_scaled)\n",
    "            glcm = graycomatrix(raster_ubyte, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "            dissimilarity.append(graycoprops(glcm, 'dissimilarity')[0, 0])\n",
    "            correlation.append(graycoprops(glcm, 'correlation')[0, 0])\n",
    "    \n",
    "    return dissimilarity, correlation\n",
    "\n",
    "# concatenate and export the features generated from the functions above\n",
    "def get_grid_tex_features(\n",
    "    xs: Union[List[float], np.ndarray],\n",
    "    ys: Union[List[float], np.ndarray],\n",
    "    grid_paths: Union[str, List[str]],\n",
    "    buffer_size: float,\n",
    "    features: List[str] = ['dissimilarity', 'correlation']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate texture features from raster grid(s) for given points using a square buffer.\n",
    "\n",
    "    Args:\n",
    "        xs (Union[List[float], np.ndarray]): X-coordinates of target points.\n",
    "        ys (Union[List[float], np.ndarray]): Y-coordinates of target points.\n",
    "        grid_paths (Union[str, List[str]]): Path(s) to raster file(s).\n",
    "        buffer_size (float): Size of the buffer in meters (half side length of the square).\n",
    "        features (List[str]): List of texture features to calculate. Options include 'contrast', \n",
    "                              'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM'.\n",
    "                              Defaults to ['dissimilarity', 'correlation'].\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing selected texture feature values for each point and raster.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the number of x and y coordinates don't match.\n",
    "        FileNotFoundError: If any of the specified raster files are not found.\n",
    "        ValueError: If an invalid feature is specified.\n",
    "    \"\"\"\n",
    "    if len(xs) != len(ys):\n",
    "        raise ValueError(\"The number of x and y coordinates must be the same\")\n",
    "\n",
    "    valid_features = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']\n",
    "    invalid_features = [f for f in features if f not in valid_features]\n",
    "    if invalid_features:\n",
    "        raise ValueError(f\"Invalid feature(s) specified: {', '.join(invalid_features)}. \"\n",
    "                         f\"Valid options are: {', '.join(valid_features)}\")\n",
    "\n",
    "    # Convert inputs to numpy arrays\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Ensure grid_paths is a list\n",
    "    if isinstance(grid_paths, str):\n",
    "        grid_paths = [grid_paths]\n",
    "\n",
    "    # Check if all files exist\n",
    "    for path in grid_paths:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Raster file not found: {path}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for grid_path in grid_paths:\n",
    "        # Open the raster file\n",
    "        raster = rxr.open_rasterio(grid_path, masked=True).squeeze()\n",
    "        bounds = raster.rio.bounds()\n",
    "        raster_np = raster.values\n",
    "\n",
    "        # Get the CRS of the raster\n",
    "        raster_crs = raster.rio.crs\n",
    "\n",
    "        # Convert coordinates to pixel space\n",
    "        x_pixels = ((xs - bounds[0]) / (bounds[2] - bounds[0]) * (raster_np.shape[1] - 1)).astype(int)\n",
    "        y_pixels = ((ys - bounds[1]) / (bounds[3] - bounds[1]) * (raster_np.shape[0] - 1)).astype(int)\n",
    "\n",
    "        # Calculate pixel size\n",
    "        if raster_crs.is_projected:\n",
    "            pixel_size_x = (bounds[2] - bounds[0]) / raster_np.shape[1] * raster_crs.linear_units_factor\n",
    "            pixel_size_y = (bounds[3] - bounds[1]) / raster_np.shape[0] * raster_crs.linear_units_factor\n",
    "        else:\n",
    "            # Approximate pixel size for geographic CRS\n",
    "            pixel_size_x = (bounds[2] - bounds[0]) / raster_np.shape[1] * 111319.9  # approx meters per degree at equator\n",
    "            pixel_size_y = (bounds[3] - bounds[1]) / raster_np.shape[0] * 111319.9\n",
    "\n",
    "        # Calculate buffer size in pixels\n",
    "        pixels_per_meter = min(1/pixel_size_x, 1/pixel_size_y)\n",
    "        buffer_pixels = max(int(buffer_size * pixels_per_meter), 1)  # Ensure at least 1 pixel\n",
    "\n",
    "        feature_results = {feature: [] for feature in features}\n",
    "\n",
    "        for x, y in zip(x_pixels, y_pixels):\n",
    "            x_min, x_max = max(0, x - buffer_pixels), min(raster_np.shape[1], x + buffer_pixels + 1)\n",
    "            y_min, y_max = max(0, y - buffer_pixels), min(raster_np.shape[0], y + buffer_pixels + 1)\n",
    "            \n",
    "            values = raster_np[y_min:y_max, x_min:x_max]\n",
    "            if not np.isnan(values).all():\n",
    "                raster_scaled = exposure.rescale_intensity(values, in_range=(np.nanmin(values), np.nanmax(values)), out_range=(0, 1))\n",
    "                raster_ubyte = util.img_as_ubyte(raster_scaled)\n",
    "                glcm = graycomatrix(raster_ubyte, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "                for feature in features:\n",
    "                    feature_results[feature].append(graycoprops(glcm, feature)[0, 0])\n",
    "            else:\n",
    "                for feature in features:\n",
    "                    feature_results[feature].append(np.nan)\n",
    "\n",
    "        # Create a prefix for column names\n",
    "        prefix = os.path.splitext(os.path.basename(grid_path))[0]\n",
    "        for feature in features:\n",
    "            results[f'{prefix}_{feature}'] = feature_results[feature]\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean in a buffer zone (circle) surrounding each target point\n",
    "def get_mean(xs, ys, raster_grad, bounds, radius):\n",
    "    points = [Point(x, y) for x, y in zip(xs, ys)]\n",
    "    means = []\n",
    "    \n",
    "    for point in points:\n",
    "        x = point.x\n",
    "        y = point.y\n",
    "        xx = []\n",
    "        yy = []\n",
    "        \n",
    "        if x < bounds[0] or x > bounds[2] or y < bounds[1] or y > bounds[3]:\n",
    "            means.append(np.nan)\n",
    "        else:\n",
    "            x_origin = math.floor((x-bounds[0])/(bounds[2]-bounds[0])*(raster_grad.shape[1]-1))\n",
    "            y_origin = math.floor((y-bounds[1])/(bounds[3]-bounds[1])*(raster_grad.shape[0]-1))\n",
    "\n",
    "            radius_ = math.ceil(radius*raster_grad.shape[1]/(bounds[2]-bounds[0]))\n",
    "            points_circle = []\n",
    "        \n",
    "            for xr in range(-radius_, radius_+1):\n",
    "                Y = int((radius_*radius_-xr*xr)**0.5)\n",
    "                for yr in range(-Y, Y+1):\n",
    "                    xc = xr + x_origin\n",
    "                    yc = yr + y_origin\n",
    "                    if xc >= 0 and xc <= raster_grad.shape[1]-1 and yc >= 0 and yc <= raster_grad.shape[0]-1:\n",
    "                        points_circle.append((xc, yc))\n",
    "                    \n",
    "            for p in points_circle:\n",
    "                xx.append(p[0])\n",
    "                yy.append(raster_grad.shape[0]-1-p[1])\n",
    "        \n",
    "            means.append(np.nanmean(raster_grad[yy, xx]))\n",
    "            \n",
    "    return means\n",
    "\n",
    "# calculate gradient\n",
    "def get_gradient_data(xs, ys, elevation_files):\n",
    "    grid_features = []\n",
    "    grid_column_names = []\n",
    "\n",
    "    for grid in tqdm(elevation_files):\n",
    "        prefix = os.path.splitext(os.path.basename(grid))[0]\n",
    "        grid_column_names.append(prefix+'_dx')\n",
    "        grid_column_names.append(prefix+'_dy')\n",
    "\n",
    "        raster = rxr.open_rasterio(grid, masked=True).squeeze()\n",
    "        \n",
    "        # reproject if required\n",
    "        if raster.rio.crs != frame_target.crs:\n",
    "            raster = raster.rio.reproject(frame_target.crs)\n",
    "\n",
    "        bounds = (raster.rio.bounds())\n",
    "        raster_np = np.array(raster)\n",
    "        raster_grad_x = np.gradient(raster_np)[1]\n",
    "        raster_grad_y = np.gradient(raster_np)[0]\n",
    "        \n",
    "        means_x = get_mean(xs, ys, raster_grad_x, bounds, 0.1)\n",
    "        means_y = get_mean(xs, ys, raster_grad_y, bounds, 0.1)\n",
    "        grid_features.append(means_x)\n",
    "        grid_features.append(means_y)\n",
    "\n",
    "        del raster\n",
    "        del raster_np\n",
    "\n",
    "    return pd.DataFrame(np.array(grid_features).T, columns=grid_column_names)\n",
    "\n",
    "deposit_elev_file = f'./Dataset/Output/training_data_deposit_elevation.csv'\n",
    "\n",
    "if os.path.isfile(deposit_elev_file):\n",
    "    print('The elevation dataset (deposits) already exists.')\n",
    "    deposit_elev_data = pd.read_csv(deposit_elev_file, index_col=False)\n",
    "else:    \n",
    "    deposit_elev_data = get_gradient_data(deposit_x, deposit_y, elevation_files)\n",
    "    deposit_elev_data = deposit_elev_data.dropna(axis=1, thresh=round(deposit_num*0.9))\n",
    "    deposit_elev_data.to_csv(deposit_elev_file, index=False)\n",
    "    print(f'The elevation dataset (deposits) has been saved to {deposit_elev_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dde1c",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Mineral Occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b15492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated using the coordinates of the mineral occurrences (positive samples)\n",
    "deposit_training_data_file = f'./Dataset/Output/training_data_deposit.csv'\n",
    "\n",
    "if os.path.isfile(deposit_training_data_file):\n",
    "    print('The training data file (deposits) already exists.')\n",
    "    deposit_training_data = pd.read_csv(deposit_training_data_file, index_col=False)    \n",
    "    deposit_training_data_columns = deposit_training_data.columns.tolist()\n",
    "    deposit_num_data_columns = []\n",
    "    deposit_cat_data_columns = []\n",
    "\n",
    "    for column in deposit_training_data_columns:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            deposit_cat_data_columns.append(column)\n",
    "        else:\n",
    "            deposit_num_data_columns.append(column)\n",
    "    \n",
    "    deposit_num_data_columns = [e for e in deposit_num_data_columns if e not in ('label', 'sample_weight')]\n",
    "    deposit_num_data = deposit_training_data[deposit_num_data_columns]\n",
    "    deposit_cat_data = deposit_training_data[deposit_cat_data_columns]\n",
    "else:\n",
    "    deposit_training_data = pd.concat([\n",
    "        deposit_coords,\n",
    "        deposit_grid_data,\n",
    "        deposit_elev_data,\n",
    "        deposit_vector_data,\n",
    "        deposit_cat_data\n",
    "    ],\n",
    "        axis=1)\n",
    "    \n",
    "    # remove the samples with missing values\n",
    "    deposit_training_data = deposit_training_data.dropna()\n",
    "\n",
    "    # separate numerical and categorical features\n",
    "    deposit_num_data = deposit_training_data[deposit_training_data.columns[4:deposit_training_data.shape[1]-deposit_cat_data.shape[1]]]\n",
    "    deposit_cat_data = deposit_training_data[deposit_training_data.columns[deposit_training_data.shape[1]-deposit_cat_data.shape[1]:deposit_training_data.shape[1]]]\n",
    "\n",
    "    unique_columns_num = []\n",
    "    # romove columns (features) with a unique value from the list of numerical features\n",
    "    for i in range(deposit_num_data.shape[1]):\n",
    "        if len(deposit_num_data.iloc[:, i].round(4).unique()) == 1:\n",
    "            unique_columns_num.append(deposit_num_data.columns[i])\n",
    "\n",
    "    deposit_num_data.drop(unique_columns_num, axis=1, inplace=True)\n",
    "    deposit_cat_data_columns = deposit_cat_data.columns.to_list()\n",
    "    deposit_features = pd.concat([deposit_num_data, deposit_cat_data], axis=1).reset_index(drop=True)\n",
    "    deposit_labels = deposit_training_data[deposit_training_data.columns[2]].reset_index(drop=True)\n",
    "    deposit_weights = deposit_training_data[deposit_training_data.columns[3]].reset_index(drop=True)\n",
    "    deposit_training_data = pd.concat([deposit_labels, deposit_weights, deposit_features], axis=1).reset_index(drop=True)\n",
    "    deposit_training_data.to_csv(deposit_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (deposits) has been saved to {deposit_training_data_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b248b724",
   "metadata": {},
   "source": [
    "### Random (Unlabelled) Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and extract the coordinates of a number of random samples within the desired polygon\n",
    "def get_unlab_samples(polygon, num_features):\n",
    "    bounds = polygon.bounds\n",
    "    \n",
    "    rand_x = np.random.uniform(low=extent[0], high=extent[1], size=deposit_training_data.shape[0]*2)\n",
    "    rand_y = np.random.uniform(low=extent[2], high=extent[3], size=deposit_training_data.shape[0]*2)\n",
    "    \n",
    "    unlab_x = []\n",
    "    unlab_y = []\n",
    "\n",
    "    for x, y in zip(rand_x, rand_y):\n",
    "        if len(unlab_x) == deposit_training_data.shape[0]:\n",
    "            break\n",
    "        p = Point((x, y))\n",
    "        if p.within(polygon.geometry[0]):\n",
    "            unlab_x.append(x)\n",
    "            unlab_y.append(y)\n",
    "    \n",
    "    return unlab_x, unlab_y\n",
    "\n",
    "num_features = deposit_training_data.shape[1] - 1\n",
    "# export the coordinates of the random samples to a CSV file\n",
    "unlab_coords_file = './Dataset/Output/training_data_unlab_coords.csv'\n",
    "\n",
    "if os.path.isfile(unlab_coords_file):\n",
    "    print('The coordinates of unlabelled samples already exist.')\n",
    "    unlab_coords = pd.read_csv(unlab_coords_file, index_col=False)\n",
    "    unlab_x = pd.Series.tolist(unlab_coords['X'])\n",
    "    unlab_y = pd.Series.tolist(unlab_coords['Y'])\n",
    "else:\n",
    "    unlab_x, unlab_y = get_unlab_samples(frame_target, num_features)\n",
    "    unlab_coords = pd.DataFrame(unlab_x, columns=['X'])\n",
    "    unlab_coords['Y'] = unlab_y\n",
    "    unlab_label = [0]*len(unlab_x)\n",
    "    unlab_coords['label'] = unlab_label\n",
    "    unlab_coords['sample_weight'] = 0.5\n",
    "    unlab_coords.to_csv(unlab_coords_file, index=False)\n",
    "    print(f'The coordinates of unlabelled samples have been saved to {unlab_coords_file}.')\n",
    "\n",
    "# plot unlabelled samples\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "frame_target.plot(ax=ax, edgecolor='black', color='none', linewidth=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=2)\n",
    "ax.scatter(unlab_x, unlab_y, color='blue', edgecolors='black', s=5, alpha=0.7)\n",
    "min_occ.plot(ax=ax, edgecolor='black', color='yellow', markersize=15, alpha=0.7)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.set_title('Unlabelled Samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77b413",
   "metadata": {},
   "source": [
    "#### Create the Training Data File of Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd064601",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab_vector_file = './Dataset/Output/training_data_unlab_vector.csv'\n",
    "\n",
    "if os.path.isfile(unlab_vector_file):\n",
    "    print('The vector dataset (unlabelled samples) already exists.')\n",
    "    unlab_vector_data = pd.read_csv(unlab_vector_file, index_col=False)\n",
    "else:\n",
    "    unlab_vector_data = get_vector_data(unlab_x, unlab_y)\n",
    "    unlab_vector_data = unlab_vector_data[unlab_vector_data.columns.intersection(unlab_vector_data.columns)]\n",
    "    unlab_vector_data.to_csv(unlab_vector_file, index=False)\n",
    "    print(f'The vector dataset (unlabelled samples) has been saved to {unlab_vector_file}.')\n",
    "\n",
    "unlab_cat_file = './Dataset/Output/training_data_unlab_categorical.csv'\n",
    "\n",
    "if os.path.isfile(unlab_cat_file):\n",
    "    print('The categorical dataset (unlabelled samples) already exists.')\n",
    "    unlab_cat_data = pd.read_csv(unlab_cat_file, index_col=False)\n",
    "else:\n",
    "    unlab_cat_data = get_cat_data(unlab_x, unlab_y)\n",
    "    unlab_cat_data = unlab_cat_data[unlab_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    unlab_cat_data.to_csv(unlab_cat_file, index=False)\n",
    "    print(f'The categorical dataset (unlabelled samples) has been saved to {unlab_cat_file}.')\n",
    "\n",
    "unlab_grid_file = './Dataset/Output/training_data_unlab_grids.csv'\n",
    "\n",
    "if os.path.isfile(unlab_grid_file):\n",
    "    print('The grid dataset (unlabelled samples) already exists.')\n",
    "    unlab_grid_data = pd.read_csv(unlab_grid_file, index_col=False)\n",
    "else:\n",
    "    unlab_grid_data = get_grid_data(unlab_x, unlab_y, grid_filenames)\n",
    "    unlab_grid_data = unlab_grid_data[unlab_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    unlab_grid_data.to_csv(unlab_grid_file, index=False)\n",
    "    print(f'The grid dataset (unlabelled samples) has been saved to {unlab_grid_file}.')\n",
    "    \n",
    "unlab_elev_file = './Dataset/Output/training_data_unlab_elevation.csv'\n",
    "\n",
    "if os.path.isfile(unlab_elev_file):\n",
    "    print('The elevation dataset (unlabelled samples) already exists.')\n",
    "    unlab_elev_data = pd.read_csv(unlab_elev_file, index_col=False)\n",
    "else:\n",
    "    unlab_elev_data = get_gradient_data(unlab_x, unlab_y, elevation_files)\n",
    "    unlab_elev_data = unlab_elev_data[unlab_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    unlab_elev_data.to_csv(unlab_elev_file, index=False)\n",
    "    print(f'The elevation dataset (unlabelled samples) has been saved to {unlab_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581b266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "unlab_training_data_file = './Dataset/Output/training_data_unlab.csv'\n",
    "\n",
    "if os.path.isfile(unlab_training_data_file):\n",
    "    print('The training data file (unlabelled samples) already exists.')\n",
    "    unlab_training_data = pd.read_csv(unlab_training_data_file, index_col=False)\n",
    "else:\n",
    "    unlab_training_data = pd.concat([\n",
    "        unlab_coords,\n",
    "        unlab_grid_data,\n",
    "        unlab_elev_data,\n",
    "        unlab_vector_data,\n",
    "        unlab_cat_data],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # remove missing values\n",
    "    unlab_training_data = unlab_training_data.dropna()\n",
    "    unlab_training_data = unlab_training_data[unlab_training_data.columns.intersection(deposit_training_data.columns)]\n",
    "    unlab_training_data.to_csv(unlab_training_data_file, index=False)\n",
    "    \n",
    "    print(f'The training data file (unlabelled samples) has been saved to {unlab_training_data_file}.')\n",
    "\n",
    "Xy_train_original_df_file = './Dataset/Output/Xy_train_original.csv'\n",
    "\n",
    "if os.path.isfile(Xy_train_original_df_file):\n",
    "    Xy_train_original_df = pd.read_csv(Xy_train_original_df_file, index_col=False)\n",
    "    print('Features file already exists!')\n",
    "    \n",
    "    with open('./Dataset/Output/st_scaler.pkl', 'rb') as f:\n",
    "        st_scaler = pickle.load(f)\n",
    "        \n",
    "    with open('./Dataset/Output/encoder.pkl', 'rb') as f:\n",
    "        enc = pickle.load(f)\n",
    "else:\n",
    "    deposit_labels = deposit_training_data['label']\n",
    "    unlab_labels = unlab_training_data['label']\n",
    "    \n",
    "    deposit_weights = deposit_training_data['sample_weight']\n",
    "    unlab_weights = unlab_training_data['sample_weight']\n",
    "    \n",
    "    labels = pd.concat([deposit_labels, unlab_labels]).reset_index(drop=True)\n",
    "    weights = pd.concat([deposit_weights, unlab_weights]).reset_index(drop=True)\n",
    "    \n",
    "    training_data_original = pd.concat([deposit_training_data, unlab_training_data]).reset_index(drop=True)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    training_data_num = training_data_original[deposit_num_data.columns]\n",
    "    training_data_cat = training_data_original[deposit_cat_data.columns]\n",
    "\n",
    "    # drop highly correlated features\n",
    "    # create a correlation matrix\n",
    "    corr_matrix = training_data_num.corr(method='spearman').abs()\n",
    "    # select the upper triangle of the correlation matrix\n",
    "    corr_upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # find features with the correlation greater than 0.7\n",
    "    corr_drop = [column for column in corr_upper.columns if any(corr_upper[column] > 0.7)]\n",
    "    print('List of the features removed due to high correlation with other features:', corr_drop)\n",
    "    # drop features\n",
    "    training_data_num_purged = training_data_num.drop(corr_drop, axis=1)\n",
    "    \n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(training_data_cat)\n",
    "    training_data_cat_encoded = enc.transform(training_data_cat).toarray()\n",
    "    training_data_cat_columns = enc.get_feature_names_out(training_data_cat.columns).tolist()\n",
    "    training_data_cat_encoded = pd.DataFrame(training_data_cat_encoded, columns=training_data_cat_columns)\n",
    "    \n",
    "    features_labels_encoded = pd.concat([training_data_num_purged, training_data_cat_encoded, weights, labels], axis=1).reset_index(drop=True)\n",
    "    features_labels_list = features_labels_encoded.columns.tolist()\n",
    "    features_list = features_labels_list.copy()\n",
    "    features_list = [e for e in features_list if e not in ('label', 'sample_weight')]\n",
    "\n",
    "    deposit_data = features_labels_encoded[features_labels_encoded['label']==1]\n",
    "    unlab_data = features_labels_encoded[features_labels_encoded['label']==0]\n",
    "\n",
    "    deposit_features = deposit_data[deposit_data.columns[:-1]]\n",
    "    unlab_features = unlab_data[unlab_data.columns[:-1]]\n",
    "\n",
    "    deposit_labels = deposit_data[deposit_data.columns[-1]]\n",
    "    unlab_labels = unlab_data[unlab_data.columns[-1]]\n",
    "\n",
    "#     deposit_weights = deposit_data[deposit_data.columns[-2]]\n",
    "#     unlab_weights = unlab_data[unlab_data.columns[-2]]\n",
    "    \n",
    "#     deposit_labels_weights = np.vstack((deposit_labels, deposit_weights))\n",
    "    \n",
    "    # train test\n",
    "    # split positive samples into training and test datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(deposit_features, deposit_labels, train_size=0.75, random_state=42)\n",
    "    X_train = np.vstack((X_train, unlab_features))\n",
    "    y_train = np.vstack((y_train.values.reshape(-1, 1), unlab_labels.values.reshape(-1, 1)))\n",
    "    \n",
    "    Xy_train_original = np.hstack((X_train, y_train))\n",
    "    Xy_train_original_df = pd.DataFrame(Xy_train_original, columns=features_labels_list)\n",
    "    Xy_train_original_df.to_csv('./Dataset/Output/Xy_train_original.csv', index=False)\n",
    "    \n",
    "    X_test_df = pd.DataFrame(X_test, columns=features_list).reset_index(drop=True)\n",
    "    y_test_df = pd.DataFrame(y_test, columns=['label']).reset_index(drop=True)\n",
    "    weights_df = pd.DataFrame(X_test, columns=['sample_weight']).reset_index(drop=True)\n",
    "    \n",
    "    X_train_num = Xy_train_original_df[training_data_num_purged.columns]\n",
    "    \n",
    "    st_scaler = StandardScaler()\n",
    "    X_train_num = st_scaler.fit_transform(X_train_num)\n",
    "    X_train_num = pd.DataFrame(X_train_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    X_test_num = X_test_df[training_data_num_purged.columns]\n",
    "    X_test_num = st_scaler.transform(X_test_num)\n",
    "    X_test_num = pd.DataFrame(X_test_num, columns=training_data_num_purged.columns)\n",
    "    \n",
    "    Xy_train = pd.concat([X_train_num, Xy_train_original_df[training_data_cat_columns], Xy_train_original_df['sample_weight'], Xy_train_original_df['label']], axis=1).reset_index(drop=True)\n",
    "    Xy_test = pd.concat([X_test_num, X_test_df[training_data_cat_columns], weights_df, y_test_df], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    Xy_train.to_csv('./Dataset/Output/Xy_train.csv', index=False)\n",
    "    Xy_test.to_csv('./Dataset/Output/Xy_test.csv', index=False)\n",
    "    \n",
    "    # save the standard scaler model\n",
    "    with open('./Dataset/Output/st_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(st_scaler, f)\n",
    "        \n",
    "    # save the encoder model\n",
    "    with open('./Dataset/Output/encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(enc, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f43089",
   "metadata": {},
   "source": [
    "### Create the Predictive Model\n",
    "\n",
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: number of trees in the foreset\n",
    "# max_features: max number of features considered for splitting a node\n",
    "# max_depth: max number of levels in each decision tree\n",
    "# min_samples_split: min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf: min number of data points allowed in a leaf node\n",
    "# bootstrap: method for sampling data points (with or without replacement)\n",
    "\n",
    "if os.path.isfile('./Dataset/Output/model.pkl'):\n",
    "    print('The model already exists!')\n",
    "    # load the model\n",
    "    with open('./Dataset/Output/model.pkl', 'rb') as f:\n",
    "        bc_best = pickle.load(f)\n",
    "else:\n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_fold = 10\n",
    "\n",
    "    Xy_train_file = f'./Dataset/Output/Xy_train.csv'\n",
    "    Xy_train = pd.read_csv(Xy_train_file, index_col=False)\n",
    "    features = Xy_train[Xy_train.columns[:-1]]\n",
    "    labels = Xy_train[Xy_train.columns[-1]]\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "\n",
    "    sample_weight_train = X_train_[X_train_.columns[-1]]\n",
    "    X_train_ = X_train_[X_train_.columns[:-1]]\n",
    "    sample_weight_test = X_test_[X_test_.columns[-1]]\n",
    "    X_test_ = X_test_[X_test_.columns[:-1]]\n",
    "\n",
    "    search_space = {\n",
    "    'base_estimator__bootstrap': Categorical([True, False]), # values for boostrap can be either True or False\n",
    "    'base_estimator__max_depth': Integer(5, 20), # values of max_depth are integers\n",
    "    'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "    'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "    'base_estimator__min_samples_split': Integer(2, 30),\n",
    "    'base_estimator__n_estimators': Integer(10, 200),\n",
    "    'max_samples': Integer(int(0.5*(len(y_train_)-sum(y_train_))), int(0.9*(len(y_train_)-sum(y_train_))))\n",
    "    }\n",
    "\n",
    "    bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50, # specify how many iterations\n",
    "                                    scoring='accuracy', n_jobs=-1, cv=n_fold, verbose=1, random_state=42, return_train_score=True)\n",
    "    bc_bayes_search.fit(X_train_, y_train_, sample_weight=sample_weight_train) # callback=on_step will print score after each iteration\n",
    "\n",
    "    X_pred = bc_bayes_search.best_estimator_.predict(X_test_)\n",
    "    X_pred_acc = accuracy_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_pre = precision_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_rec = recall_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_f1 = f1_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "\n",
    "    bc_best = bc_bayes_search.best_estimator_\n",
    "    bc_best_acc = bc_bayes_search.best_score_\n",
    "    print('The highest accuracy during cross validation:', bc_best_acc)\n",
    "    print('Accuracy:', X_pred_acc)\n",
    "    print('Precision:', X_pred_pre)\n",
    "    print('Recall:', X_pred_rec)\n",
    "    print('F1-Score:', X_pred_f1)\n",
    "    \n",
    "    # save the model\n",
    "    with open('./Dataset/Output/model.pkl', 'wb') as f:\n",
    "        pickle.dump(bc_best, f)\n",
    "    \n",
    "    importances = []\n",
    "    estimators = bc_best.estimators_\n",
    "    importances = [estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))]\n",
    "    importances = np.hstack(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(1)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(1))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe5d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test = pd.read_csv('./Dataset/Output/Xy_test.csv', index_col=False)\n",
    "X_test = Xy_test[Xy_test.columns[:-2]]\n",
    "y_test = Xy_test[Xy_test.columns[-1]]\n",
    "sample_weight_test = Xy_test[Xy_test.columns[-2]]\n",
    "X_pred = bc_best.predict(X_test)\n",
    "X_pred_acc = accuracy_score(y_test, X_pred, sample_weight=sample_weight_test)\n",
    "print('Accuracy:', X_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a6220f",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_file = './Dataset/Output/features_importance.csv'\n",
    "\n",
    "if os.path.isfile(features_importance_file):\n",
    "    features_importance = pd.read_csv(features_importance_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    output_features = Xy_train_original_df.columns.tolist()\n",
    "    output_features = [e for e in output_features if e not in ('label', 'sample_weight')]\n",
    "    \n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    features_importance = [(feature, round(importance, 5)) for feature, importance in zip(output_features, importances_mean)]\n",
    "    features_importance = sorted(features_importance, key=lambda x:x[1], reverse=True)\n",
    "    features_importance_df = pd.DataFrame(features_importance, columns=['Feature', 'Importance'])\n",
    "    features_importance_df['Variance'] = importances_var\n",
    "    features_importance_df.to_csv(features_importance_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e63d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in features_importance]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(features_importance)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b776f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "features_importance_ = features_importance[:20]\n",
    "features_importance_.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in features_importance_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in features_importance_]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c76bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_df = pd.read_csv('./Dataset/Output/Xy_train.csv', index_col=False)\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_df.columns):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_facecolor('whitesmoke')\n",
    "    ax2 = ax1.twiny()\n",
    "    \n",
    "    ax1.hist(Xy_train_original_df[feature], bins=25, alpha=0.0)\n",
    "\n",
    "    h1 = ax2.hist(Xy_train_df.loc[Xy_train_df['label']==0][feature], bins=25, color='LightSalmon', label='Negative')\n",
    "    h2 = ax2.hist(Xy_train_df.loc[Xy_train_df['label']==1][feature], bins=25, color='DarkSeaGreen', label='Positive', alpha=0.8)\n",
    "\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_train_df_pivot = Xy_train_df.pivot(columns=['label'])\n",
    "Xy_train_original_df_pivot = Xy_train_original_df.pivot(columns=['label'])\n",
    "\n",
    "nb_groups1 = Xy_train_df['label'].nunique()\n",
    "nb_groups2 = Xy_train_original_df['label'].nunique()\n",
    "\n",
    "@interact\n",
    "def show_map(feature=Xy_train_df.columns):\n",
    "    bplot1 = [Xy_train_df_pivot[feature][var].dropna() for var in Xy_train_df_pivot[feature]]\n",
    "    bplot2 = [Xy_train_original_df_pivot[feature][var].dropna() for var in Xy_train_original_df_pivot[feature]]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "    box_param1 = dict(whis=(5, 95), widths=0.2, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='black', fillstyle=None),\n",
    "                      medianprops=dict(color='black'), boxprops=dict(facecolor='tab:blue'))\n",
    "    box_param2 = dict(whis=(5, 95), widths=0, patch_artist=True,\n",
    "                      flierprops=dict(marker='.', markeredgecolor='none', fillstyle=None),\n",
    "                      medianprops=dict(color='none'), whiskerprops=dict(color='none'),\n",
    "                      boxprops=dict(facecolor='none', edgecolor='none'))\n",
    "\n",
    "    ax1.boxplot(bplot1, positions=np.arange(nb_groups1), **box_param1)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.boxplot(bplot2, positions=np.arange(nb_groups2), **box_param2)\n",
    "\n",
    "    # format x ticks\n",
    "    labelsize = 12\n",
    "    ax1.set_xticks(np.arange(nb_groups1))\n",
    "    ax1.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax1.tick_params(axis='x', labelsize=labelsize)\n",
    "\n",
    "    # format y ticks\n",
    "    yticks_fmt = dict(axis='y', labelsize=labelsize)\n",
    "\n",
    "    # format axes labels\n",
    "    label_fmt = dict(size=12, labelpad=15)\n",
    "    ax1.set_xlabel(feature, **label_fmt)\n",
    "    ax1.set_ylabel(feature + '\\n(Standardised)', **label_fmt)\n",
    "    ax2.set_ylabel(feature + '\\n(Actual)', **label_fmt)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046cc81b",
   "metadata": {},
   "source": [
    "### Generate Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the sampling resolution\n",
    "size_x = 0.1\n",
    "size_y = 0.1\n",
    "\n",
    "range_x = np.arange(extent_target[0], extent_target[1], size_x)\n",
    "range_y = np.arange(extent_target[2], extent_target[3], size_y)\n",
    "\n",
    "num_x = len(range_x)\n",
    "num_y = len(range_y)\n",
    "\n",
    "xs, ys = np.meshgrid(range_x, range_y)\n",
    "\n",
    "target_coords_file = './Dataset/Output/target_coords.csv'\n",
    "target_mask_file = './Dataset/Output/target_mask.csv'\n",
    "\n",
    "# export the coordinates of the target points and create a mask to keep the points only inside the target polygon boundaries\n",
    "if os.path.isfile(target_coords_file) and os.path.isfile(target_mask_file):\n",
    "    print('The coordinates of target points already exist.')\n",
    "    target_coords = pd.read_csv(target_coords_file, index_col=False)\n",
    "    target_x = target_coords['X']\n",
    "    target_y = target_coords['Y']\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    target_x = []\n",
    "    target_y = []\n",
    "    target_mask = []\n",
    "    for xx, yy in zip(xs.flatten(), ys.flatten()):\n",
    "        p = Point((xx, yy))\n",
    "        if p.within(frame_target.geometry[0]):\n",
    "            target_x.append(xx)\n",
    "            target_y.append(yy)\n",
    "            target_mask.append(True)\n",
    "        else:\n",
    "            target_mask.append(False)\n",
    "    \n",
    "    target_coords = pd.DataFrame(target_x, columns=['X'])\n",
    "    target_coords['Y'] = target_y\n",
    "    target_coords.to_csv(target_coords_file, index=False)\n",
    "    \n",
    "    mask_x = np.array([xs.flatten()]).T\n",
    "    mask_y = np.array([ys.flatten()]).T\n",
    "    target_mask_ = np.array([target_mask]).T\n",
    "    target_mask = np.hstack((mask_x, mask_y, target_mask_))\n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "    print('The coordinates of target points and mask have been saved.')\n",
    "\n",
    "print(f'Number of samples: ', len(target_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.scatter(target_x, target_y, color='black', edgecolors='none', s=2)\n",
    "frame_Lchn.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20e8fe",
   "metadata": {},
   "source": [
    "#### Extract Values of Features at Target Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector_file = './Dataset/Output/target_vector.csv'\n",
    "\n",
    "if os.path.isfile(target_vector_file):\n",
    "    print('The vector dataset (target points) already exists.')\n",
    "    target_vector_data = pd.read_csv(target_vector_file, index_col=False)\n",
    "else:\n",
    "    target_vector_data = get_vector_data(target_x, target_y)\n",
    "    target_vector_data = target_vector_data[target_vector_data.columns.intersection(deposit_vector_data.columns)]\n",
    "    target_vector_data.to_csv(target_vector_file, index=False)\n",
    "    print(f'The vector dataset (target points) has been saved to {target_vector_file}.')\n",
    "\n",
    "target_cat_file = './Dataset/Output/target_categorical.csv'\n",
    "\n",
    "if os.path.isfile(target_cat_file):\n",
    "    print('The categorical dataset (target points) already exists.')\n",
    "    target_cat_data = pd.read_csv(target_cat_file, index_col=False)\n",
    "else:\n",
    "    target_cat_data = get_cat_data(target_x, target_y)\n",
    "    target_cat_data = target_cat_data[target_cat_data.columns.intersection(deposit_cat_data.columns)]\n",
    "    target_cat_data.to_csv(target_cat_file, index=False)\n",
    "    print(f'The categorical dataset (target points) has been saved to {target_cat_file}.')\n",
    "\n",
    "target_grid_file = './Dataset/Output/target_grids.csv'\n",
    "\n",
    "if os.path.isfile(target_grid_file):\n",
    "    print('The grid dataset (target points) already exists.')\n",
    "    target_grid_data = pd.read_csv(target_grid_file, index_col=False)\n",
    "else:\n",
    "    target_grid_data = get_grid_data(target_x, target_y, grid_filenames)\n",
    "    target_grid_data = target_grid_data[target_grid_data.columns.intersection(deposit_grid_data.columns)]\n",
    "    target_grid_data.to_csv(target_grid_file, index=False)\n",
    "    print(f'The grid dataset (target points) has been saved to {target_grid_file}.')\n",
    "    \n",
    "target_elev_file = './Dataset/Output/target_elevation.csv'\n",
    "\n",
    "if os.path.isfile(target_elev_file):\n",
    "    print('The elevation dataset (target points) already exists.')\n",
    "    target_elev_data = pd.read_csv(target_elev_file, index_col=False)\n",
    "else: \n",
    "    target_elev_data = get_gradient_data(target_x, target_y, elevation_files)\n",
    "    target_elev_data = target_elev_data[target_elev_data.columns.intersection(deposit_elev_data.columns)]\n",
    "    target_elev_data.to_csv(target_elev_file, index=False)\n",
    "    print(f'The elevation dataset (target poitns) has been saved to {target_elev_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2b063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features generated in the previous cell\n",
    "target_data_file = './Dataset/Output/target_data.csv'\n",
    "\n",
    "if os.path.isfile(target_data_file):\n",
    "    print('The target data file already exists.')\n",
    "    target_data = pd.read_csv(target_data_file, index_col=False)\n",
    "else:\n",
    "    target_data = pd.concat([target_coords,\n",
    "                             target_grid_data,\n",
    "                             target_elev_data,\n",
    "                             target_vector_data,\n",
    "                             target_cat_data],\n",
    "                            axis=1)\n",
    "    target_data.to_csv(target_data_file, index=False)\n",
    "    print(f'The target data file has been saved to {target_data_file}.')\n",
    "\n",
    "target_features_file = './Dataset/Output/target_features.csv'\n",
    "\n",
    "if os.path.isfile(target_features_file):\n",
    "    print('The target features file already exists.')\n",
    "    target_features = pd.read_csv(target_features_file, index_col=False)\n",
    "    target_coords_purged = pd.read_csv('./Dataset/Output/target_coords_purged.csv', index_col=False)\n",
    "    target_mask = genfromtxt(target_mask_file, delimiter=',')\n",
    "else:\n",
    "    index_null = target_data[target_data.isnull().any(axis=1)].index.to_list()\n",
    "\n",
    "    for index in index_null:\n",
    "        for i in range(target_mask.shape[0]):\n",
    "            if target_data['X'][index] == target_mask[i, 0] and target_data['Y'][index] == target_mask[i, 1]:\n",
    "                target_mask[i, 2] = False\n",
    "                \n",
    "    np.savetxt(target_mask_file, target_mask, delimiter=',')\n",
    "\n",
    "    # remove missing values\n",
    "    target_data = target_data.dropna()\n",
    "    target_coords_purged = target_data[['X', 'Y']]\n",
    "    target_coords_purged.to_csv('./Dataset/Output/target_coords_purged.csv', index=False)\n",
    "\n",
    "    features_label_list = Xy_train_original_df.columns.tolist()\n",
    "    features_list = features_label_list.copy()\n",
    "    features_list = [e for e in features_list if e not in ('label', 'sample_weight')]\n",
    "    num_features_list = []\n",
    "    cat_features_list = []\n",
    "\n",
    "    for column in features_list:\n",
    "        if column.startswith('Intrusions_') or column.startswith('MetamorphicFacies') or column.startswith('RockUnits'):\n",
    "            cat_features_list.append(column)\n",
    "        else:\n",
    "            num_features_list.append(column)\n",
    "    \n",
    "    # separate numerical and categorical features\n",
    "    target_data_num = target_data[num_features_list]\n",
    "    target_data_cat = target_data[deposit_cat_data_columns]\n",
    "\n",
    "    st_scaler.fit(target_data_num)\n",
    "    target_features_num = st_scaler.transform(target_data_num)\n",
    "    target_features_cat = enc.transform(target_data_cat).toarray()\n",
    "    target_features = np.hstack((target_features_num, target_features_cat))\n",
    "    target_features = pd.DataFrame(target_features, columns=features_list)\n",
    "    target_features.to_csv(target_features_file, index=False)\n",
    "    print(f'The target features file has been saved to {target_data_file}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962160f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ_clipped = min_occ.clip(frame_target)\n",
    "\n",
    "@interact(feature=target_features.columns.tolist())\n",
    "def show_map(feature):\n",
    "    feature_values = []\n",
    "    count = 0\n",
    "\n",
    "    for mask in target_mask[:, 2]:\n",
    "        if mask:\n",
    "            feature_values.append(target_features.iloc[count][feature])\n",
    "            count += 1\n",
    "        else:\n",
    "            feature_values.append(np.nan)\n",
    "\n",
    "    feature_values_2d = np.reshape(feature_values, (num_y, num_x))\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    min_occ_clipped.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=30, alpha=0.5)\n",
    "    frame_target.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "    cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "    plt.yticks(rotation=90, va='center')\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "\n",
    "    cb = plt.imshow(feature_values_2d, cmap='Spectral_r', origin='lower', interpolation='bilinear', extent=extent_target)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "    fig.add_axes(cax)\n",
    "    plt.colorbar(cb, orientation='horizontal', label=feature, cax=cax)\n",
    "    ax.set_title(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b0c4e",
   "metadata": {},
   "source": [
    "### Calculate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92284f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs_file = './Dataset/Output/target_probs.csv'\n",
    "\n",
    "if os.path.isfile(target_probs_file):\n",
    "    print('The probability file already exists.')\n",
    "    target_probs = pd.read_csv(target_probs_file, index_col=False)\n",
    "else:\n",
    "    probs = bc_best.predict_proba(target_features)\n",
    "    \n",
    "    mm_scaler = MinMaxScaler()\n",
    "    probs_scaled = mm_scaler.fit_transform(probs[:, 1].reshape(-1, 1))\n",
    "    \n",
    "    target_probs = target_coords_purged.reset_index().copy()\n",
    "    target_probs['prob'] = probs_scaled\n",
    "    target_probs.to_csv(target_probs_file, index=False)\n",
    "    print(f'The probability file has been saved to {target_probs_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557397a",
   "metadata": {},
   "source": [
    "#### Plot Prospectivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae198dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the probability map\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cb = plt.scatter(target_probs['X'], target_probs['Y'], 30, c=target_probs['prob'], cmap='Spectral_r')\n",
    "min_occ_clipped.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=30, alpha=0.7)\n",
    "\n",
    "frame_target.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability map using the target points\n",
    "probs_temp = []\n",
    "count = 0\n",
    "\n",
    "for mask in target_mask[:, 2]:\n",
    "    if mask:\n",
    "        probs_temp.append(target_probs['prob'][count])\n",
    "        count += 1\n",
    "    else:\n",
    "        probs_temp.append(np.nan)\n",
    "\n",
    "probs_2d = np.reshape(probs_temp, (num_y, num_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "min_occ_clipped.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=30, alpha=0.7)\n",
    "frame_target.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "plt.imshow(probs_2d, cmap='Spectral_r', origin='lower', interpolation='bilinear', extent=extent_target)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title(f'Mineral Prospectivity Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad998194",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2d_ud = np.flipud(probs_2d)\n",
    "\n",
    "# export the map to a GeoTIFF file\n",
    "xmin, ymin, xmax, ymax = [min(range_x), min(range_y), max(range_x), max(range_y)]\n",
    "geotransform = (xmin, 0.05, 0, ymax, 0, -0.05)\n",
    "map_file = './Dataset/Output/probability_map_v1.1.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "dataset = driver.Create(map_file, num_x, num_y, 1, gdal.GDT_Float32)\n",
    "dataset.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4283)\n",
    "dataset.SetProjection(srs.ExportToWkt())\n",
    "dataset.GetRasterBand(1).WriteArray(probs_2d_ud)\n",
    "dataset.FlushCache()\n",
    "dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae1cc1",
   "metadata": {},
   "source": [
    "### Select Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e67ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_important = []\n",
    "\n",
    "for i in range(len(features_importance)):\n",
    "    if features_importance[i][1] >= 0.02:\n",
    "        features_important.append(features_importance[i][0])\n",
    "\n",
    "Xy_train_important_file = './Dataset/Output/Xy_train_important.csv'\n",
    "Xy_test_important_file = './Dataset/Output/Xy_test_important.csv'\n",
    "\n",
    "if os.path.isfile(Xy_train_important_file) and os.path.isfile(Xy_test_important_file):\n",
    "    print('The training data file already exists.')\n",
    "    Xy_train_important = pd.read_csv(Xy_train_important_file, index_col=False)\n",
    "    Xy_test_important = pd.read_csv(Xy_test_important_file, index_col=False)\n",
    "else:\n",
    "    features_important.append('sample_weight')\n",
    "    features_important.append('label')\n",
    "    # features_important.remove()\n",
    "\n",
    "    Xy_train_file = './Dataset/Output/Xy_train.csv'\n",
    "    Xy_test_file = './Dataset/Output/Xy_test.csv'\n",
    "\n",
    "    Xy_train = pd.read_csv(Xy_train_file, index_col=False)\n",
    "    Xy_test = pd.read_csv(Xy_test_file, index_col=False)\n",
    "\n",
    "    Xy_train_important = Xy_train[features_important]\n",
    "    Xy_test_important = Xy_test[features_important]\n",
    "\n",
    "    Xy_train_important.to_csv('./Dataset/Output/Xy_train_important.csv', index=False)\n",
    "    Xy_test_important.to_csv('./Dataset/Output/Xy_test_important.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea01b7",
   "metadata": {},
   "source": [
    "### Create the Predictive Model\n",
    "\n",
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators: number of trees in the foreset\n",
    "# max_features: max number of features considered for splitting a node\n",
    "# max_depth: max number of levels in each decision tree\n",
    "# min_samples_split: min number of data points placed in a node before the node is split\n",
    "# min_samples_leaf: min number of data points allowed in a leaf node\n",
    "# bootstrap: method for sampling data points (with or without replacement)\n",
    "\n",
    "model_important_file = './Dataset/Output/model_important.pkl'\n",
    "\n",
    "if os.path.isfile(model_important_file):\n",
    "#if 0:\n",
    "    print('The model already exists!')\n",
    "    # load the model\n",
    "    with open(model_important_file, 'rb') as f:\n",
    "        bc_best_important = pickle.load(f)\n",
    "else:\n",
    "    # Random Forest model structure\n",
    "    rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "    bc = BaggingPuClassifier(rf, n_jobs=-1, random_state=42)\n",
    "\n",
    "    n_fold = 10\n",
    "\n",
    "    Xy_train_file = f'./Dataset/Output/Xy_train_important.csv'\n",
    "    Xy_train = pd.read_csv(Xy_train_file, index_col=False)\n",
    "    features = Xy_train[Xy_train.columns[:-1]]\n",
    "    labels = Xy_train[Xy_train.columns[-1]]\n",
    "    X_train_, X_test_, y_train_, y_test_ = train_test_split(features, labels, train_size=0.75, random_state=42)\n",
    "    \n",
    "    sample_weight_train = X_train_[X_train_.columns[-1]]\n",
    "    X_train_ = X_train_[X_train_.columns[:-1]]\n",
    "    sample_weight_test = X_test_[X_test_.columns[-1]]\n",
    "    X_test_ = X_test_[X_test_.columns[:-1]]\n",
    "\n",
    "    search_space = {\n",
    "    'base_estimator__bootstrap': Categorical([True, False]), # values for boostrap can be either True or False\n",
    "    'base_estimator__max_depth': Integer(5, 20), # values of max_depth are integers\n",
    "    'base_estimator__max_features': Categorical([None, 'sqrt','log2']), \n",
    "    'base_estimator__min_samples_leaf': Integer(2, 20),\n",
    "    'base_estimator__min_samples_split': Integer(2, 30),\n",
    "    'base_estimator__n_estimators': Integer(10, 200),\n",
    "    'max_samples': Integer(int(0.5*(len(y_train_)-sum(y_train_))), int(0.9*(len(y_train_)-sum(y_train_))))\n",
    "    }\n",
    "\n",
    "    bc_bayes_search = BayesSearchCV(bc, search_space, n_iter=50, # specify how many iterations\n",
    "                                    scoring='accuracy', n_jobs=-1, cv=n_fold, verbose=1, random_state=42, return_train_score=True)\n",
    "    bc_bayes_search.fit(X_train_, y_train_, sample_weight=sample_weight_train) # callback=on_step will print score after each iteration\n",
    "\n",
    "    X_pred = bc_bayes_search.best_estimator_.predict(X_test_)\n",
    "    X_pred_acc = accuracy_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_pre = precision_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_rec = recall_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    X_pred_f1 = f1_score(y_test_, X_pred, sample_weight=sample_weight_test)\n",
    "    \n",
    "    bc_best_important = bc_bayes_search.best_estimator_\n",
    "    bc_best_important_acc = bc_bayes_search.best_score_\n",
    "    \n",
    "    print('The highest accuracy during cross validation:', bc_best_important_acc)\n",
    "    print('Accuracy:', X_pred_acc)\n",
    "    print('Precision:', X_pred_pre)\n",
    "    print('Recall:', X_pred_rec)\n",
    "    print('F1-Score:', X_pred_f1)\n",
    "\n",
    "    # save the model\n",
    "    with open(model_important_file, 'wb') as f:\n",
    "        pickle.dump(bc_best_important, f)\n",
    "    \n",
    "    importances = []\n",
    "    estimators = bc_best_important.estimators_\n",
    "    importances = [estimators[j].feature_importances_.reshape(-1, 1) for j in range(len(estimators))]\n",
    "    importances = np.hstack(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171fc6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_best_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_estimated = []\n",
    "for pair in bc_best_important.oob_decision_function_:\n",
    "    if np.isnan(pair[0]) or pair[0] < pair[1]:\n",
    "        labels_estimated.append(1)\n",
    "    else:\n",
    "        labels_estimated.append(0)\n",
    "        \n",
    "print('Number of positive samples', labels_estimated.count(1))\n",
    "print('Number of negative samples', labels_estimated.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_test_important = pd.read_csv('./Dataset/Output/Xy_test_important.csv', index_col=False)\n",
    "X_test = Xy_test_important[Xy_test_important.columns[:-2]]\n",
    "y_test = Xy_test_important[Xy_test_important.columns[-1]]\n",
    "sample_weight_test = Xy_test[Xy_test.columns[-2]]\n",
    "X_pred = bc_best_important.predict(X_test)\n",
    "X_pred_acc = accuracy_score(y_test, X_pred, sample_weight=sample_weight_test)\n",
    "print('Accuracy:', X_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8ffc0",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d27fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importance_important_file = './Dataset/Output/features_importance_important.csv'\n",
    "\n",
    "if os.path.isfile(features_importance_important_file):\n",
    "    features_importance_important = pd.read_csv(features_importance_important_file, index_col=False).to_numpy().tolist()\n",
    "else:\n",
    "    importances_mean = importances.mean(axis=1)\n",
    "    importances_var = importances.var(axis=1)\n",
    "\n",
    "    features_important = [e for e in features_important if e not in ('label', 'sample_weight')]\n",
    "    features_importance_important = [(feature, round(importance, 5)) for feature, importance in zip(features_important, importances_mean)]\n",
    "        \n",
    "    features_importance_important = sorted(features_importance_important, key=lambda x:x[1], reverse=True)\n",
    "    features_importance_important_df = pd.DataFrame(features_importance_important, columns=['Feature', 'Importance'])\n",
    "    features_importance_important_df['Variance'] = importances_var\n",
    "    features_importance_important_df.to_csv(features_importance_important_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34701d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in features_importance_important]\n",
    "# cumulative importance\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "\n",
    "x_values = list(range(len(features_importance_important)))\n",
    "x_values = [x+1 for x in x_values]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax1 = ax2.twinx()\n",
    "\n",
    "ax2.bar(x_values, sorted_importances, edgecolor='gray', facecolor='LightSalmon', width=1, alpha=0.5)\n",
    "ax1.plot(x_values, cumulative_importances, 'k--')\n",
    "\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlim(0.5, len(cumulative_importances)+0.5)\n",
    "\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "ax2.set_ylabel('Feature Importance')\n",
    "ax1.set_ylabel('Cumulative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print significant features above some threshold\n",
    "features_importance_ = features_importance_important[:20]\n",
    "features_importance_.sort(key=lambda x:x[1])\n",
    "ft_imps = [x[1] for x in features_importance_]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "ax.set_facecolor('whitesmoke')\n",
    "bar = ax.barh(range(len(ft_imps)), ft_imps)\n",
    "\n",
    "def gradientbars(bars, data):\n",
    "    ax = bars[0].axes\n",
    "    lim = ax.get_xlim()+ax.get_ylim()\n",
    "    for bar in bars:\n",
    "        bar.set_zorder(1)\n",
    "        bar.set_facecolor('none')\n",
    "        bar.set_edgecolor('black')\n",
    "        x, y = bar.get_xy()\n",
    "        w, h = bar.get_width(), bar.get_height()\n",
    "        cmap = plt.get_cmap('winter')\n",
    "        grad = np.atleast_2d(np.linspace(0, 1*w/max(data), 256))\n",
    "        ax.imshow(grad, extent=[x, x+w, y, y+h], aspect='auto', zorder=0, norm=mpl.colors.NoNorm(vmin=0, vmax=1), cmap=cmap, alpha=0.8)\n",
    "        manual_labels = [x[0] for x in features_importance_]\n",
    "        ax.set_yticks(np.arange(0, len(data), 1).tolist())\n",
    "        ax.set_yticklabels(manual_labels, minor=False)\n",
    "    ax.axis(lim)\n",
    "\n",
    "gradientbars(bar, ft_imps)\n",
    "plt.gca().yaxis.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a924bc",
   "metadata": {},
   "source": [
    "### Calculate Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_probs_important_file = './Dataset/Output/target_probs_important.csv'\n",
    "\n",
    "if os.path.isfile(target_probs_important_file):\n",
    "    print('The probability file already exists.')\n",
    "    target_probs_important = pd.read_csv(target_probs_important_file, index_col=False)\n",
    "else:\n",
    "    try:\n",
    "        features_important = [e for e in features_important if e not in ('label', 'sample_weight')]\n",
    "        probs = bc_best_important.predict_proba(target_features[features_important])\n",
    "    except:\n",
    "        probs = bc_best_important.predict_proba(target_features[features_important])\n",
    "    \n",
    "    mm_scaler = MinMaxScaler()\n",
    "    probs_scaled = mm_scaler.fit_transform(probs[:, 1].reshape(-1, 1))\n",
    "    \n",
    "    target_probs_important = target_coords_purged.reset_index().copy()\n",
    "    target_probs_important['prob'] = probs_scaled\n",
    "    target_probs_important.to_csv(target_probs_important_file, index=False)\n",
    "    print(f'The probability file has been saved to {target_probs_important_file}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf13d1",
   "metadata": {},
   "source": [
    "#### Plot Prospectivity Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occ_clipped = min_occ.clip(frame_target)\n",
    "\n",
    "# plot the probability map\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cb = plt.scatter(target_probs_important['X'], target_probs_important['Y'], 30, c=target_probs_important['prob'], cmap='Spectral_r')\n",
    "min_occ_clipped.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=30, alpha=0.7)\n",
    "\n",
    "frame_target.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "# colorbar\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aeb6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability map using the target points\n",
    "probs_temp = []\n",
    "count = 0\n",
    "\n",
    "for mask in target_mask[:, 2]:\n",
    "    if mask:\n",
    "        probs_temp.append(target_probs_important['prob'][count])\n",
    "        count += 1\n",
    "    else:\n",
    "        probs_temp.append(np.nan)\n",
    "\n",
    "probs_2d = np.reshape(probs_temp, (num_y, num_x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "min_occ_clipped.plot(ax=ax, marker='X', edgecolor='black', color='yellow', markersize=30, alpha=0.7)\n",
    "frame_target.plot(ax=ax, edgecolor='red', color='none', linewidth=1)\n",
    "cx.add_basemap(ax, crs='EPSG:4283', source=cx.providers.Esri.WorldGrayCanvas)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "plt.yticks(rotation=90, va='center')\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "plt.imshow(probs_2d, cmap='Spectral_r', origin='lower', interpolation='bilinear', extent=extent_target)\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.new_vertical(size='5%', pad=0.5, pack_start=True)\n",
    "fig.add_axes(cax)\n",
    "plt.colorbar(cb, orientation='horizontal', label='Probability', cax=cax)\n",
    "ax.set_title(f'Mineral Prospectivity Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff631bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_2d_ud = np.flipud(probs_2d)\n",
    "\n",
    "# export the map to a GeoTIFF file\n",
    "xmin, ymin, xmax, ymax = [min(range_x), min(range_y), max(range_x), max(range_y)]\n",
    "geotransform = (xmin, 0.05, 0, ymax, 0, -0.05)\n",
    "map_file = './Dataset/Output/probability_map_important_v1.1.tif'\n",
    "driver = gdal.GetDriverByName('GTiff')\n",
    "dataset = driver.Create(map_file, num_x, num_y, 1, gdal.GDT_Float32)\n",
    "dataset.SetGeoTransform(geotransform)\n",
    "srs = osr.SpatialReference()\n",
    "srs.ImportFromEPSG(4283)\n",
    "dataset.SetProjection(srs.ExportToWkt())\n",
    "dataset.GetRasterBand(1).WriteArray(probs_2d_ud)\n",
    "dataset.FlushCache()\n",
    "dataset = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
